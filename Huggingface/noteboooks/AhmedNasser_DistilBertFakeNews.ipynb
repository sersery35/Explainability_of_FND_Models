{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'3.7.13'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "# !pip install shap==0.39\n",
    "# !pip install transformers==4.5\n",
    "python_version()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "from Huggingface.utils import HuggingfaceDatasetManager, DatasetTypeEnum, explain_text, TransformersModelTypeEnum\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/253M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7475ff01f6c4902aae3208dbd9d1e30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ahmednasser/DistilBert-FakeNews were not used when initializing DistilBertForSequenceClassification: ['bert.transformer.layer.4.ffn.lin1.weight', 'bert.transformer.layer.4.attention.v_lin.weight', 'bert.embeddings.LayerNorm.weight', 'bert.transformer.layer.1.ffn.lin2.weight', 'bert.transformer.layer.0.attention.v_lin.weight', 'bert.transformer.layer.5.ffn.lin2.bias', 'bert.transformer.layer.4.attention.out_lin.weight', 'bert.transformer.layer.1.attention.v_lin.bias', 'bert.transformer.layer.2.attention.q_lin.weight', 'bert.transformer.layer.2.output_layer_norm.weight', 'bert.transformer.layer.4.ffn.lin1.bias', 'bert.transformer.layer.3.ffn.lin1.weight', 'bert.transformer.layer.0.attention.out_lin.bias', 'bert.transformer.layer.3.attention.out_lin.weight', 'bert.transformer.layer.0.ffn.lin1.bias', 'bert.transformer.layer.3.output_layer_norm.weight', 'bert.transformer.layer.2.attention.q_lin.bias', 'bert.transformer.layer.5.attention.k_lin.bias', 'bert.transformer.layer.5.attention.v_lin.bias', 'bert.transformer.layer.2.sa_layer_norm.weight', 'bert.transformer.layer.5.attention.v_lin.weight', 'bert.transformer.layer.5.output_layer_norm.weight', 'classifier.2.bias', 'bert.transformer.layer.0.sa_layer_norm.weight', 'bert.transformer.layer.3.sa_layer_norm.bias', 'bert.transformer.layer.0.attention.v_lin.bias', 'bert.transformer.layer.3.attention.v_lin.bias', 'bert.transformer.layer.5.output_layer_norm.bias', 'bert.embeddings.position_embeddings.weight', 'bert.transformer.layer.4.attention.q_lin.weight', 'bert.transformer.layer.3.ffn.lin2.weight', 'bert.transformer.layer.0.ffn.lin2.bias', 'bert.transformer.layer.3.attention.k_lin.bias', 'bert.transformer.layer.0.attention.out_lin.weight', 'bert.transformer.layer.1.output_layer_norm.bias', 'classifier.2.weight', 'bert.transformer.layer.2.ffn.lin2.bias', 'bert.embeddings.LayerNorm.bias', 'bert.transformer.layer.0.attention.k_lin.bias', 'bert.transformer.layer.2.attention.out_lin.bias', 'bert.transformer.layer.2.attention.k_lin.weight', 'bert.transformer.layer.1.attention.k_lin.bias', 'bert.transformer.layer.0.attention.k_lin.weight', 'bert.transformer.layer.3.attention.v_lin.weight', 'bert.transformer.layer.3.attention.out_lin.bias', 'bert.transformer.layer.4.attention.out_lin.bias', 'bert.transformer.layer.4.output_layer_norm.bias', 'bert.transformer.layer.0.attention.q_lin.weight', 'bert.transformer.layer.5.attention.out_lin.bias', 'bert.transformer.layer.4.ffn.lin2.weight', 'classifier.0.weight', 'bert.transformer.layer.1.ffn.lin1.weight', 'bert.transformer.layer.5.attention.q_lin.bias', 'classifier.0.bias', 'bert.transformer.layer.3.attention.q_lin.bias', 'bert.transformer.layer.1.attention.k_lin.weight', 'bert.transformer.layer.1.sa_layer_norm.weight', 'bert.transformer.layer.3.attention.q_lin.weight', 'bert.transformer.layer.0.output_layer_norm.weight', 'bert.transformer.layer.0.ffn.lin2.weight', 'bert.transformer.layer.1.ffn.lin2.bias', 'bert.transformer.layer.2.attention.v_lin.bias', 'bert.transformer.layer.4.attention.q_lin.bias', 'bert.transformer.layer.2.ffn.lin1.bias', 'bert.transformer.layer.1.sa_layer_norm.bias', 'bert.transformer.layer.4.attention.k_lin.bias', 'bert.transformer.layer.5.attention.k_lin.weight', 'bert.transformer.layer.2.ffn.lin1.weight', 'bert.transformer.layer.0.ffn.lin1.weight', 'bert.transformer.layer.2.sa_layer_norm.bias', 'bert.transformer.layer.4.attention.k_lin.weight', 'bert.transformer.layer.4.ffn.lin2.bias', 'bert.transformer.layer.5.attention.out_lin.weight', 'bert.transformer.layer.5.sa_layer_norm.bias', 'bert.transformer.layer.4.output_layer_norm.weight', 'bert.transformer.layer.0.output_layer_norm.bias', 'bert.transformer.layer.1.ffn.lin1.bias', 'bert.transformer.layer.1.attention.v_lin.weight', 'bert.transformer.layer.1.attention.q_lin.weight', 'bert.transformer.layer.2.output_layer_norm.bias', 'bert.transformer.layer.4.sa_layer_norm.weight', 'bert.transformer.layer.5.ffn.lin2.weight', 'bert.transformer.layer.5.ffn.lin1.bias', 'bert.transformer.layer.4.sa_layer_norm.bias', 'bert.transformer.layer.2.attention.out_lin.weight', 'bert.transformer.layer.5.attention.q_lin.weight', 'bert.transformer.layer.0.sa_layer_norm.bias', 'bert.transformer.layer.2.attention.k_lin.bias', 'bert.transformer.layer.1.output_layer_norm.weight', 'bert.transformer.layer.3.attention.k_lin.weight', 'bert.transformer.layer.1.attention.out_lin.weight', 'bert.transformer.layer.1.attention.out_lin.bias', 'bert.transformer.layer.1.attention.q_lin.bias', 'bert.transformer.layer.4.attention.v_lin.bias', 'bert.transformer.layer.3.sa_layer_norm.weight', 'bert.embeddings.word_embeddings.weight', 'bert.transformer.layer.2.attention.v_lin.weight', 'bert.transformer.layer.0.attention.q_lin.bias', 'bert.transformer.layer.3.output_layer_norm.bias', 'bert.transformer.layer.5.sa_layer_norm.weight', 'bert.transformer.layer.3.ffn.lin2.bias', 'bert.transformer.layer.2.ffn.lin2.weight', 'bert.transformer.layer.3.ffn.lin1.bias', 'bert.transformer.layer.5.ffn.lin1.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ahmednasser/DistilBert-FakeNews and are newly initialized: ['transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.2.attention.v_lin.bias', 'embeddings.position_embeddings.weight', 'transformer.layer.3.attention.q_lin.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.5.attention.q_lin.bias', 'pre_classifier.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.3.output_layer_norm.weight', 'classifier.bias', 'classifier.weight', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.bias', 'embeddings.word_embeddings.weight', 'transformer.layer.4.ffn.lin1.bias', 'embeddings.LayerNorm.bias', 'transformer.layer.1.ffn.lin2.weight', 'pre_classifier.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.2.attention.q_lin.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Permission denied (os error 13)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_451991/466463861.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mmodel_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_type\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'NAME'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAutoModelForSequenceClassification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextClassificationPipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_all_scores\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbinary_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0membedding_dim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistillbert\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mword_embeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding_dim\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/anaconda3/envs/huggingface2/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    553\u001B[0m                     \u001B[0;34mf\"Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    554\u001B[0m                 )\n\u001B[0;32m--> 555\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mtokenizer_class\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    556\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    557\u001B[0m         \u001B[0;31m# Otherwise we have to be creative.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/anaconda3/envs/huggingface2/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1789\u001B[0m             \u001B[0muse_auth_token\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1790\u001B[0m             \u001B[0mcache_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1791\u001B[0;31m             \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1792\u001B[0m         )\n\u001B[1;32m   1793\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/anaconda3/envs/huggingface2/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36m_from_pretrained\u001B[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1927\u001B[0m         \u001B[0;31m# Instantiate tokenizer.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1928\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1929\u001B[0;31m             \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minit_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0minit_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1930\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1931\u001B[0m             raise OSError(\n",
      "\u001B[0;32m~/Applications/anaconda3/envs/huggingface2/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert_fast.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, vocab_file, tokenizer_file, do_lower_case, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001B[0m\n\u001B[1;32m    186\u001B[0m             \u001B[0mtokenize_chinese_chars\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtokenize_chinese_chars\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m             \u001B[0mstrip_accents\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstrip_accents\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 188\u001B[0;31m             \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    189\u001B[0m         )\n\u001B[1;32m    190\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Applications/anaconda3/envs/huggingface2/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    107\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mfast_tokenizer_file\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mfrom_slow\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;31m# We have a serialization from tokenizers which let us directly build the backend\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 109\u001B[0;31m             \u001B[0mfast_tokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTokenizerFast\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfast_tokenizer_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    110\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mslow_tokenizer\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m             \u001B[0;31m# We need to convert a slow tokenizer to build the backend\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mException\u001B[0m: Permission denied (os error 13)"
     ]
    }
   ],
   "source": [
    "model_type = TransformersModelTypeEnum.AN_DISTIL_BERT_FAKE_NEWS\n",
    "model_name = model_type.value['NAME']\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, binary_output=True, device=0)\n",
    "embedding_dim = model.distillbert.embeddings.word_embeddings.embedding_dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the respective dataset for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset is unknown\n",
    "dataset_type = DatasetTypeEnum.ROBERTA_FAKE_NEWS\n",
    "dataset_manager = HuggingfaceDatasetManager(dataset_type, embedding_dim=embedding_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_samples = dataset_manager.fetch_true_samples(sample_count=10)\n",
    "true_sample_shap_values = explain_text(model_type=TransformersModelTypeEnum.AN_DISTILL_BERT_FAKE_NEWS, pipeline=pipeline, text=true_samples[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}